{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:ndp]",
      "language": "python",
      "name": "conda-env-ndp-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Day100_transfer_learning_HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9G94lSd2oft",
        "colab_type": "text"
      },
      "source": [
        "## 作業\n",
        "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
        "\n",
        "\n",
        "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
        "\n",
        "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vc-_CDKnAwG",
        "colab_type": "text"
      },
      "source": [
        "本日作業的解答是參考 Keras 官方的範例 - 使用 ResNet 訓練 Cifar-10 資料集，共使用了以下技巧\n",
        "\n",
        "資料增強\n",
        "使用 ResNet 網路架構 (此處沒有使用 transfer learning，因為沒有使用 pre-trained 的 weights)\n",
        "學習率動態調整\n",
        "使用 generator 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baQWJphNeWXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "# config = tf.ConfigProto()\n",
        "\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "# sess = tf.Session(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDqW_gKZeWdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c3eabb9-c05d-49b1-f0ca-80867d3baab1"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "#Trains a ResNet on the CIFAR10 dataset.\n",
        "\n",
        "ResNet 共有兩個版本，此處解答我們使用 v1 來做訓練。\n",
        "ResNet v1:\n",
        "[Deep Residual Learning for Image Recognition\n",
        "](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "ResNet v2:\n",
        "[Identity Mappings in Deep Residual Networks\n",
        "](https://arxiv.org/pdf/1603.05027.pdf)\n",
        "\"\"\"\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 訓練用的超參數\n",
        "batch_size = 128  \n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# 資料標準化的方式，此處使用減去所有影像的平均值\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 9 # 使用 ResNet-56 的網路架構\n",
        "\n",
        "# 使用的 ResNet 模型版本\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# 計算不同 ResNet 版本對應的網路深度，此處都是根據 paper 的定義來計算\n",
        "depth = n * 6 + 2\n",
        "\n",
        "# 模型的名稱\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# 讀取 Cifar-10 資料集\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 影像輸入的維度\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# 先把影像縮放到 0-1 之間\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# 再減去所有影像的平均值\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean \n",
        "    x_test -= x_train_mean # 此處要注意！測試資料也是減去訓練資料的平均值來做標準化，不可以減測試資料的平均值 (因為理論上你是不能知道測試資料的平均值的！)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# 對 label 做 one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# 學習率動態調整。當跑到第幾個 epcoh 時，根據設定修改學習率。這邊的數值都是參考原 paper\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "# 使用 resnet_layer 來建立我們的 ResNet 模型\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    # 建立卷積層\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    # 對輸入進行卷機，根據 conv_first 來決定 conv. bn, activation 的順序\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "# Resnet v1 共有三個 stage，每經過一次 stage，影像就會變小一半，但 channels 數量增加一倍。ResNet-20 代表共有 20 層 layers，疊越深參數越多\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # 模型的初始設置，要用多少 filters，共有幾個 residual block （組成 ResNet 的單元）\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "    \n",
        "    # 建立 Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    # 先對影像做第一次卷機\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    \n",
        "    # 總共建立 3 個 stage\n",
        "    for stack in range(3):\n",
        "        # 每個 stage 建立數個 residual blocks (數量視你的層數而訂，越多層越多 block)\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y]) # 此處把 featuremaps 與 上一層的輸入加起來 (欲更了解結構需閱讀原論文)\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # 建立分類\n",
        "    # 使用 average pooling，且 size 跟 featuremaps 的 size 一樣 （相等於做 GlobalAveragePooling）\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    \n",
        "    # 接上 Dense layer 來做分類\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # 建立模型\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# 建立 ResNet v1 模型\n",
        "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "# 編譯模型，使用 Adam 優化器並使用學習率動態調整的函數，０代表在第一個 epochs\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# 使用動態調整學習率\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# 使用自動降低學習率 (當 validation loss 連續 5 次沒有下降時，自動降低學習率)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "# 設定 callbacks\n",
        "callbacks = [lr_reducer, lr_scheduler]\n",
        "\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "datagen = ImageDataGenerator(\n",
        "    # set input mean to 0 over the dataset\n",
        "    featurewise_center=False,\n",
        "    # set each sample mean to 0\n",
        "    samplewise_center=False,\n",
        "    # divide inputs by std of dataset\n",
        "    featurewise_std_normalization=False,\n",
        "    # divide each input by its std\n",
        "    samplewise_std_normalization=False,\n",
        "    # apply ZCA whitening\n",
        "    zca_whitening=False,\n",
        "    # epsilon for ZCA whitening\n",
        "    zca_epsilon=1e-06,\n",
        "    # randomly rotate images in the range (deg 0 to 180)\n",
        "    rotation_range=0,\n",
        "    # randomly shift images horizontally\n",
        "    width_shift_range=0.1,\n",
        "    # randomly shift images vertically\n",
        "    height_shift_range=0.1,\n",
        "    # set range for random shear\n",
        "    shear_range=0.,\n",
        "    # set range for random zoom\n",
        "    zoom_range=0.,\n",
        "    # set range for random channel shifts\n",
        "    channel_shift_range=0.,\n",
        "    # set mode for filling points outside the input boundaries\n",
        "    fill_mode='nearest',\n",
        "    # value used for fill_mode = \"constant\"\n",
        "    cval=0.,\n",
        "    # randomly flip images\n",
        "    horizontal_flip=True,\n",
        "    # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    # set rescaling factor (applied before any other transformation)\n",
        "    rescale=None,\n",
        "    # set function that will be applied on each input\n",
        "    preprocessing_function=None,\n",
        "    # image data format, either \"channels_first\" or \"channels_last\"\n",
        "    data_format=None,\n",
        "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "    validation_split=0.0)\n",
        "\n",
        "# 將資料送進 ImageDataGenrator 中做增強\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "Learning rate:  0.001\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 32, 32, 16)   0           activation_10[0][0]              \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 16)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2320        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 32, 32, 16)   0           activation_12[0][0]              \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 16)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 16)   2320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 16)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 16)   2320        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 16)   64          conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 32, 32, 16)   0           activation_14[0][0]              \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 16)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 16)   2320        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 16)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 16)   2320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 32, 32, 16)   64          conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 32, 32, 16)   0           activation_16[0][0]              \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 16)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 32)   4640        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 32)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 32)   544         activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 32)   0           conv2d_21[0][0]                  \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 32)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 32)   0           activation_20[0][0]              \n",
            "                                                                 batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 32)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 16, 32)   0           activation_22[0][0]              \n",
            "                                                                 batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 32)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9248        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 32)   128         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 32)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 32)   0           activation_24[0][0]              \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 32)   0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9248        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 32)   0           activation_26[0][0]              \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 32)   9248        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 32)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 32)   9248        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 32)   0           activation_28[0][0]              \n",
            "                                                                 batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 32)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 16, 16, 32)   9248        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 32)   128         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 32)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 32)   9248        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 16, 16, 32)   0           activation_30[0][0]              \n",
            "                                                                 batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 32)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 32)   9248        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 32)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 32)   9248        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 32)   128         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 16, 16, 32)   0           activation_32[0][0]              \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 32)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 32)   9248        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 32)   128         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 32)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 32)   9248        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 16, 16, 32)   0           activation_34[0][0]              \n",
            "                                                                 batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 32)   0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 64)     18496       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 64)     0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 64)     2112        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 8, 8, 64)     0           conv2d_40[0][0]                  \n",
            "                                                                 batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 64)     0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 8, 8, 64)     0           activation_38[0][0]              \n",
            "                                                                 batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 64)     0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 64)     36928       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 64)     256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 64)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36928       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 8, 8, 64)     0           activation_40[0][0]              \n",
            "                                                                 batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 64)     0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 64)     36928       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 8, 64)     256         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 64)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 8, 8, 64)     36928       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 8, 64)     256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 8, 8, 64)     0           activation_42[0][0]              \n",
            "                                                                 batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 64)     0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 8, 8, 64)     36928       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 64)     256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 8, 64)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 8, 8, 64)     36928       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 8, 8, 64)     256         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 8, 8, 64)     0           activation_44[0][0]              \n",
            "                                                                 batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 64)     0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 8, 8, 64)     36928       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 64)     256         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 64)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 8, 8, 64)     36928       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 64)     256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 8, 8, 64)     0           activation_46[0][0]              \n",
            "                                                                 batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 64)     0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 8, 8, 64)     36928       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 64)     256         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 64)     0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 64)     36928       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 64)     256         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 64)     0           activation_48[0][0]              \n",
            "                                                                 batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 64)     0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 64)     36928       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 64)     256         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 64)     0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 64)     36928       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 64)     0           activation_50[0][0]              \n",
            "                                                                 batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 64)     0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 64)     36928       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 64)     256         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 64)     0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 64)     36928       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 64)     256         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 64)     0           activation_52[0][0]              \n",
            "                                                                 batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 64)     0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 861,770\n",
            "Trainable params: 857,706\n",
            "Non-trainable params: 4,064\n",
            "__________________________________________________________________________________________________\n",
            "ResNet56v1\n",
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciUprgCgmlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "216a8963-8e22-48f3-a9ea-be216d7358b2"
      },
      "source": [
        "# 訓練模型囉！\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=int(len(x_train)//batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# 評估我們的模型\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-eadf85d4d3ad>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Learning rate:  0.001\n",
            "Epoch 1/200\n",
            "390/390 [==============================] - 37s 94ms/step - loss: 1.9659 - accuracy: 0.4408 - val_loss: 2.1448 - val_accuracy: 0.3918\n",
            "Learning rate:  0.001\n",
            "Epoch 2/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 1.5226 - accuracy: 0.5917 - val_loss: 1.8562 - val_accuracy: 0.5165\n",
            "Learning rate:  0.001\n",
            "Epoch 3/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 1.2895 - accuracy: 0.6731 - val_loss: 1.5911 - val_accuracy: 0.5850\n",
            "Learning rate:  0.001\n",
            "Epoch 4/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 1.1441 - accuracy: 0.7179 - val_loss: 1.4957 - val_accuracy: 0.6240\n",
            "Learning rate:  0.001\n",
            "Epoch 5/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 1.0373 - accuracy: 0.7506 - val_loss: 1.7461 - val_accuracy: 0.6167\n",
            "Learning rate:  0.001\n",
            "Epoch 6/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.9660 - accuracy: 0.7711 - val_loss: 1.2754 - val_accuracy: 0.6961\n",
            "Learning rate:  0.001\n",
            "Epoch 7/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.9064 - accuracy: 0.7891 - val_loss: 1.4526 - val_accuracy: 0.6503\n",
            "Learning rate:  0.001\n",
            "Epoch 8/200\n",
            "390/390 [==============================] - 36s 92ms/step - loss: 0.8601 - accuracy: 0.8012 - val_loss: 1.2184 - val_accuracy: 0.7014\n",
            "Learning rate:  0.001\n",
            "Epoch 9/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.8164 - accuracy: 0.8141 - val_loss: 1.1907 - val_accuracy: 0.7039\n",
            "Learning rate:  0.001\n",
            "Epoch 10/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.7816 - accuracy: 0.8231 - val_loss: 1.1459 - val_accuracy: 0.7037\n",
            "Learning rate:  0.001\n",
            "Epoch 11/200\n",
            "390/390 [==============================] - 36s 92ms/step - loss: 0.7562 - accuracy: 0.8288 - val_loss: 1.0207 - val_accuracy: 0.7589\n",
            "Learning rate:  0.001\n",
            "Epoch 12/200\n",
            "390/390 [==============================] - 37s 94ms/step - loss: 0.7182 - accuracy: 0.8393 - val_loss: 0.9418 - val_accuracy: 0.7704\n",
            "Learning rate:  0.001\n",
            "Epoch 13/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.6998 - accuracy: 0.8461 - val_loss: 1.4931 - val_accuracy: 0.6651\n",
            "Learning rate:  0.001\n",
            "Epoch 14/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.6768 - accuracy: 0.8509 - val_loss: 0.9998 - val_accuracy: 0.7683\n",
            "Learning rate:  0.001\n",
            "Epoch 15/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.6572 - accuracy: 0.8580 - val_loss: 0.8866 - val_accuracy: 0.7894\n",
            "Learning rate:  0.001\n",
            "Epoch 16/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.6407 - accuracy: 0.8628 - val_loss: 0.9794 - val_accuracy: 0.7545\n",
            "Learning rate:  0.001\n",
            "Epoch 17/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.6307 - accuracy: 0.8638 - val_loss: 0.9747 - val_accuracy: 0.7479\n",
            "Learning rate:  0.001\n",
            "Epoch 18/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.6140 - accuracy: 0.8703 - val_loss: 0.9664 - val_accuracy: 0.7810\n",
            "Learning rate:  0.001\n",
            "Epoch 19/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.6014 - accuracy: 0.8746 - val_loss: 0.9955 - val_accuracy: 0.7706\n",
            "Learning rate:  0.001\n",
            "Epoch 20/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.5979 - accuracy: 0.8738 - val_loss: 0.9280 - val_accuracy: 0.7824\n",
            "Learning rate:  0.001\n",
            "Epoch 21/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5829 - accuracy: 0.8790 - val_loss: 0.8988 - val_accuracy: 0.7894\n",
            "Learning rate:  0.001\n",
            "Epoch 22/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5729 - accuracy: 0.8826 - val_loss: 0.8196 - val_accuracy: 0.8063\n",
            "Learning rate:  0.001\n",
            "Epoch 23/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5646 - accuracy: 0.8833 - val_loss: 0.9393 - val_accuracy: 0.7927\n",
            "Learning rate:  0.001\n",
            "Epoch 24/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5567 - accuracy: 0.8876 - val_loss: 0.8569 - val_accuracy: 0.8039\n",
            "Learning rate:  0.001\n",
            "Epoch 25/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5526 - accuracy: 0.8893 - val_loss: 1.0851 - val_accuracy: 0.7475\n",
            "Learning rate:  0.001\n",
            "Epoch 26/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5380 - accuracy: 0.8926 - val_loss: 1.1969 - val_accuracy: 0.7253\n",
            "Learning rate:  0.001\n",
            "Epoch 27/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5361 - accuracy: 0.8927 - val_loss: 0.8110 - val_accuracy: 0.8180\n",
            "Learning rate:  0.001\n",
            "Epoch 28/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5255 - accuracy: 0.8972 - val_loss: 0.9943 - val_accuracy: 0.7508\n",
            "Learning rate:  0.001\n",
            "Epoch 29/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5197 - accuracy: 0.8993 - val_loss: 0.7185 - val_accuracy: 0.8373\n",
            "Learning rate:  0.001\n",
            "Epoch 30/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.5137 - accuracy: 0.9001 - val_loss: 0.8042 - val_accuracy: 0.8265\n",
            "Learning rate:  0.001\n",
            "Epoch 31/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.5086 - accuracy: 0.9019 - val_loss: 0.8165 - val_accuracy: 0.8125\n",
            "Learning rate:  0.001\n",
            "Epoch 32/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.5088 - accuracy: 0.9019 - val_loss: 0.8130 - val_accuracy: 0.8222\n",
            "Learning rate:  0.001\n",
            "Epoch 33/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4983 - accuracy: 0.9058 - val_loss: 0.7150 - val_accuracy: 0.8441\n",
            "Learning rate:  0.001\n",
            "Epoch 34/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4979 - accuracy: 0.9055 - val_loss: 0.7212 - val_accuracy: 0.8388\n",
            "Learning rate:  0.001\n",
            "Epoch 35/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4924 - accuracy: 0.9082 - val_loss: 0.8572 - val_accuracy: 0.8103\n",
            "Learning rate:  0.001\n",
            "Epoch 36/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4841 - accuracy: 0.9100 - val_loss: 0.7526 - val_accuracy: 0.8327\n",
            "Learning rate:  0.001\n",
            "Epoch 37/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.4830 - accuracy: 0.9102 - val_loss: 0.8692 - val_accuracy: 0.8031\n",
            "Learning rate:  0.001\n",
            "Epoch 38/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4791 - accuracy: 0.9097 - val_loss: 1.0468 - val_accuracy: 0.7718\n",
            "Learning rate:  0.001\n",
            "Epoch 39/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4740 - accuracy: 0.9131 - val_loss: 0.7964 - val_accuracy: 0.8242\n",
            "Learning rate:  0.001\n",
            "Epoch 40/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4710 - accuracy: 0.9151 - val_loss: 0.8350 - val_accuracy: 0.8250\n",
            "Learning rate:  0.001\n",
            "Epoch 41/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4693 - accuracy: 0.9144 - val_loss: 0.9257 - val_accuracy: 0.8036\n",
            "Learning rate:  0.001\n",
            "Epoch 42/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4582 - accuracy: 0.9185 - val_loss: 0.7221 - val_accuracy: 0.8444\n",
            "Learning rate:  0.001\n",
            "Epoch 43/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4593 - accuracy: 0.9184 - val_loss: 0.7255 - val_accuracy: 0.8429\n",
            "Learning rate:  0.001\n",
            "Epoch 44/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4540 - accuracy: 0.9188 - val_loss: 0.6815 - val_accuracy: 0.8539\n",
            "Learning rate:  0.001\n",
            "Epoch 45/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4527 - accuracy: 0.9203 - val_loss: 0.8184 - val_accuracy: 0.8235\n",
            "Learning rate:  0.001\n",
            "Epoch 46/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4536 - accuracy: 0.9204 - val_loss: 0.7569 - val_accuracy: 0.8286\n",
            "Learning rate:  0.001\n",
            "Epoch 47/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4496 - accuracy: 0.9206 - val_loss: 0.8060 - val_accuracy: 0.8307\n",
            "Learning rate:  0.001\n",
            "Epoch 48/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4494 - accuracy: 0.9208 - val_loss: 0.8799 - val_accuracy: 0.8201\n",
            "Learning rate:  0.001\n",
            "Epoch 49/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4472 - accuracy: 0.9210 - val_loss: 0.8962 - val_accuracy: 0.8054\n",
            "Learning rate:  0.001\n",
            "Epoch 50/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4447 - accuracy: 0.9215 - val_loss: 0.7475 - val_accuracy: 0.8393\n",
            "Learning rate:  0.001\n",
            "Epoch 51/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4369 - accuracy: 0.9246 - val_loss: 0.6984 - val_accuracy: 0.8506\n",
            "Learning rate:  0.001\n",
            "Epoch 52/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4370 - accuracy: 0.9252 - val_loss: 0.6733 - val_accuracy: 0.8621\n",
            "Learning rate:  0.001\n",
            "Epoch 53/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4311 - accuracy: 0.9275 - val_loss: 0.7670 - val_accuracy: 0.8351\n",
            "Learning rate:  0.001\n",
            "Epoch 54/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4308 - accuracy: 0.9259 - val_loss: 0.8100 - val_accuracy: 0.8188\n",
            "Learning rate:  0.001\n",
            "Epoch 55/200\n",
            "390/390 [==============================] - 34s 88ms/step - loss: 0.4277 - accuracy: 0.9268 - val_loss: 0.6978 - val_accuracy: 0.8539\n",
            "Learning rate:  0.001\n",
            "Epoch 56/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4297 - accuracy: 0.9263 - val_loss: 0.8678 - val_accuracy: 0.8171\n",
            "Learning rate:  0.001\n",
            "Epoch 57/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4179 - accuracy: 0.9312 - val_loss: 0.6890 - val_accuracy: 0.8613\n",
            "Learning rate:  0.001\n",
            "Epoch 58/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4208 - accuracy: 0.9292 - val_loss: 0.6968 - val_accuracy: 0.8531\n",
            "Learning rate:  0.001\n",
            "Epoch 59/200\n",
            "390/390 [==============================] - 34s 88ms/step - loss: 0.4224 - accuracy: 0.9292 - val_loss: 0.7939 - val_accuracy: 0.8317\n",
            "Learning rate:  0.001\n",
            "Epoch 60/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4199 - accuracy: 0.9289 - val_loss: 0.9180 - val_accuracy: 0.8061\n",
            "Learning rate:  0.001\n",
            "Epoch 61/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4162 - accuracy: 0.9296 - val_loss: 0.6748 - val_accuracy: 0.8625\n",
            "Learning rate:  0.001\n",
            "Epoch 62/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4179 - accuracy: 0.9311 - val_loss: 0.6894 - val_accuracy: 0.8519\n",
            "Learning rate:  0.001\n",
            "Epoch 63/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4191 - accuracy: 0.9310 - val_loss: 0.7992 - val_accuracy: 0.8300\n",
            "Learning rate:  0.001\n",
            "Epoch 64/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4137 - accuracy: 0.9314 - val_loss: 0.7014 - val_accuracy: 0.8575\n",
            "Learning rate:  0.001\n",
            "Epoch 65/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4109 - accuracy: 0.9322 - val_loss: 0.8590 - val_accuracy: 0.8214\n",
            "Learning rate:  0.001\n",
            "Epoch 66/200\n",
            "390/390 [==============================] - 34s 88ms/step - loss: 0.4154 - accuracy: 0.9310 - val_loss: 0.7351 - val_accuracy: 0.8484\n",
            "Learning rate:  0.001\n",
            "Epoch 67/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4048 - accuracy: 0.9348 - val_loss: 0.6532 - val_accuracy: 0.8651\n",
            "Learning rate:  0.001\n",
            "Epoch 68/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.4062 - accuracy: 0.9336 - val_loss: 0.6607 - val_accuracy: 0.8684\n",
            "Learning rate:  0.001\n",
            "Epoch 69/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4073 - accuracy: 0.9339 - val_loss: 0.6851 - val_accuracy: 0.8607\n",
            "Learning rate:  0.001\n",
            "Epoch 70/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4038 - accuracy: 0.9348 - val_loss: 0.8950 - val_accuracy: 0.8170\n",
            "Learning rate:  0.001\n",
            "Epoch 71/200\n",
            "390/390 [==============================] - 35s 88ms/step - loss: 0.4041 - accuracy: 0.9340 - val_loss: 0.8161 - val_accuracy: 0.8302\n",
            "Learning rate:  0.001\n",
            "Epoch 72/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.4066 - accuracy: 0.9341 - val_loss: 1.0800 - val_accuracy: 0.7831\n",
            "Learning rate:  0.001\n",
            "Epoch 73/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3999 - accuracy: 0.9368 - val_loss: 0.7278 - val_accuracy: 0.8387\n",
            "Learning rate:  0.001\n",
            "Epoch 74/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3968 - accuracy: 0.9364 - val_loss: 0.7523 - val_accuracy: 0.8443\n",
            "Learning rate:  0.001\n",
            "Epoch 75/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3901 - accuracy: 0.9382 - val_loss: 0.9171 - val_accuracy: 0.8096\n",
            "Learning rate:  0.001\n",
            "Epoch 76/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3972 - accuracy: 0.9366 - val_loss: 0.8182 - val_accuracy: 0.8325\n",
            "Learning rate:  0.001\n",
            "Epoch 77/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3950 - accuracy: 0.9371 - val_loss: 0.6653 - val_accuracy: 0.8629\n",
            "Learning rate:  0.001\n",
            "Epoch 78/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3917 - accuracy: 0.9365 - val_loss: 0.7126 - val_accuracy: 0.8558\n",
            "Learning rate:  0.001\n",
            "Epoch 79/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.3905 - accuracy: 0.9374 - val_loss: 0.9548 - val_accuracy: 0.8130\n",
            "Learning rate:  0.001\n",
            "Epoch 80/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3922 - accuracy: 0.9379 - val_loss: 0.6729 - val_accuracy: 0.8634\n",
            "Learning rate:  0.001\n",
            "Epoch 81/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.3891 - accuracy: 0.9384 - val_loss: 0.7561 - val_accuracy: 0.8430\n",
            "Learning rate:  0.0001\n",
            "Epoch 82/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.3184 - accuracy: 0.9653 - val_loss: 0.4899 - val_accuracy: 0.9130\n",
            "Learning rate:  0.0001\n",
            "Epoch 83/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.2924 - accuracy: 0.9729 - val_loss: 0.4848 - val_accuracy: 0.9157\n",
            "Learning rate:  0.0001\n",
            "Epoch 84/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2793 - accuracy: 0.9765 - val_loss: 0.4817 - val_accuracy: 0.9175\n",
            "Learning rate:  0.0001\n",
            "Epoch 85/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2689 - accuracy: 0.9793 - val_loss: 0.4864 - val_accuracy: 0.9189\n",
            "Learning rate:  0.0001\n",
            "Epoch 86/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2624 - accuracy: 0.9799 - val_loss: 0.4931 - val_accuracy: 0.9178\n",
            "Learning rate:  0.0001\n",
            "Epoch 87/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2542 - accuracy: 0.9828 - val_loss: 0.5012 - val_accuracy: 0.9172\n",
            "Learning rate:  0.0001\n",
            "Epoch 88/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2485 - accuracy: 0.9839 - val_loss: 0.4914 - val_accuracy: 0.9196\n",
            "Learning rate:  0.0001\n",
            "Epoch 89/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.2428 - accuracy: 0.9845 - val_loss: 0.5023 - val_accuracy: 0.9179\n",
            "Learning rate:  0.0001\n",
            "Epoch 90/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.2376 - accuracy: 0.9854 - val_loss: 0.5129 - val_accuracy: 0.9166\n",
            "Learning rate:  0.0001\n",
            "Epoch 91/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.2343 - accuracy: 0.9853 - val_loss: 0.5108 - val_accuracy: 0.9166\n",
            "Learning rate:  0.0001\n",
            "Epoch 92/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2301 - accuracy: 0.9860 - val_loss: 0.5045 - val_accuracy: 0.9206\n",
            "Learning rate:  0.0001\n",
            "Epoch 93/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.2248 - accuracy: 0.9871 - val_loss: 0.5032 - val_accuracy: 0.9201\n",
            "Learning rate:  0.0001\n",
            "Epoch 94/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.2194 - accuracy: 0.9891 - val_loss: 0.5156 - val_accuracy: 0.9170\n",
            "Learning rate:  0.0001\n",
            "Epoch 95/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.2176 - accuracy: 0.9882 - val_loss: 0.5263 - val_accuracy: 0.9184\n",
            "Learning rate:  0.0001\n",
            "Epoch 96/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.2143 - accuracy: 0.9880 - val_loss: 0.5207 - val_accuracy: 0.9158\n",
            "Learning rate:  0.0001\n",
            "Epoch 97/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.2091 - accuracy: 0.9898 - val_loss: 0.5249 - val_accuracy: 0.9168\n",
            "Learning rate:  0.0001\n",
            "Epoch 98/200\n",
            "390/390 [==============================] - 36s 92ms/step - loss: 0.2069 - accuracy: 0.9904 - val_loss: 0.5130 - val_accuracy: 0.9229\n",
            "Learning rate:  0.0001\n",
            "Epoch 99/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.2043 - accuracy: 0.9905 - val_loss: 0.5209 - val_accuracy: 0.9182\n",
            "Learning rate:  0.0001\n",
            "Epoch 100/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1998 - accuracy: 0.9911 - val_loss: 0.5309 - val_accuracy: 0.9159\n",
            "Learning rate:  0.0001\n",
            "Epoch 101/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1995 - accuracy: 0.9904 - val_loss: 0.5138 - val_accuracy: 0.9200\n",
            "Learning rate:  0.0001\n",
            "Epoch 102/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1955 - accuracy: 0.9917 - val_loss: 0.5488 - val_accuracy: 0.9161\n",
            "Learning rate:  0.0001\n",
            "Epoch 103/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1936 - accuracy: 0.9915 - val_loss: 0.5390 - val_accuracy: 0.9185\n",
            "Learning rate:  0.0001\n",
            "Epoch 104/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1904 - accuracy: 0.9918 - val_loss: 0.5488 - val_accuracy: 0.9159\n",
            "Learning rate:  0.0001\n",
            "Epoch 105/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1904 - accuracy: 0.9914 - val_loss: 0.5488 - val_accuracy: 0.9179\n",
            "Learning rate:  0.0001\n",
            "Epoch 106/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1853 - accuracy: 0.9926 - val_loss: 0.5684 - val_accuracy: 0.9139\n",
            "Learning rate:  0.0001\n",
            "Epoch 107/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1848 - accuracy: 0.9925 - val_loss: 0.5618 - val_accuracy: 0.9175\n",
            "Learning rate:  0.0001\n",
            "Epoch 108/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1831 - accuracy: 0.9918 - val_loss: 0.5542 - val_accuracy: 0.9167\n",
            "Learning rate:  0.0001\n",
            "Epoch 109/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1803 - accuracy: 0.9923 - val_loss: 0.5624 - val_accuracy: 0.9163\n",
            "Learning rate:  0.0001\n",
            "Epoch 110/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1776 - accuracy: 0.9931 - val_loss: 0.5482 - val_accuracy: 0.9160\n",
            "Learning rate:  0.0001\n",
            "Epoch 111/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1764 - accuracy: 0.9932 - val_loss: 0.5332 - val_accuracy: 0.9219\n",
            "Learning rate:  0.0001\n",
            "Epoch 112/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1764 - accuracy: 0.9926 - val_loss: 0.5735 - val_accuracy: 0.9101\n",
            "Learning rate:  0.0001\n",
            "Epoch 113/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1738 - accuracy: 0.9934 - val_loss: 0.5536 - val_accuracy: 0.9166\n",
            "Learning rate:  0.0001\n",
            "Epoch 114/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1712 - accuracy: 0.9938 - val_loss: 0.5325 - val_accuracy: 0.9181\n",
            "Learning rate:  0.0001\n",
            "Epoch 115/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1694 - accuracy: 0.9940 - val_loss: 0.5457 - val_accuracy: 0.9178\n",
            "Learning rate:  0.0001\n",
            "Epoch 116/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1703 - accuracy: 0.9931 - val_loss: 0.5410 - val_accuracy: 0.9166\n",
            "Learning rate:  0.0001\n",
            "Epoch 117/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1662 - accuracy: 0.9938 - val_loss: 0.5507 - val_accuracy: 0.9186\n",
            "Learning rate:  0.0001\n",
            "Epoch 118/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1657 - accuracy: 0.9937 - val_loss: 0.5710 - val_accuracy: 0.9152\n",
            "Learning rate:  0.0001\n",
            "Epoch 119/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1639 - accuracy: 0.9940 - val_loss: 0.5531 - val_accuracy: 0.9154\n",
            "Learning rate:  0.0001\n",
            "Epoch 120/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1644 - accuracy: 0.9931 - val_loss: 0.5536 - val_accuracy: 0.9175\n",
            "Learning rate:  0.0001\n",
            "Epoch 121/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1610 - accuracy: 0.9939 - val_loss: 0.5683 - val_accuracy: 0.9116\n",
            "Learning rate:  1e-05\n",
            "Epoch 122/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1569 - accuracy: 0.9954 - val_loss: 0.5338 - val_accuracy: 0.9204\n",
            "Learning rate:  1e-05\n",
            "Epoch 123/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1549 - accuracy: 0.9960 - val_loss: 0.5283 - val_accuracy: 0.9208\n",
            "Learning rate:  1e-05\n",
            "Epoch 124/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1540 - accuracy: 0.9969 - val_loss: 0.5326 - val_accuracy: 0.9216\n",
            "Learning rate:  1e-05\n",
            "Epoch 125/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1542 - accuracy: 0.9963 - val_loss: 0.5270 - val_accuracy: 0.9217\n",
            "Learning rate:  1e-05\n",
            "Epoch 126/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1531 - accuracy: 0.9964 - val_loss: 0.5289 - val_accuracy: 0.9214\n",
            "Learning rate:  1e-05\n",
            "Epoch 127/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1518 - accuracy: 0.9975 - val_loss: 0.5275 - val_accuracy: 0.9221\n",
            "Learning rate:  1e-05\n",
            "Epoch 128/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1530 - accuracy: 0.9966 - val_loss: 0.5296 - val_accuracy: 0.9212\n",
            "Learning rate:  1e-05\n",
            "Epoch 129/200\n",
            "390/390 [==============================] - 34s 88ms/step - loss: 0.1508 - accuracy: 0.9973 - val_loss: 0.5291 - val_accuracy: 0.9233\n",
            "Learning rate:  1e-05\n",
            "Epoch 130/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1518 - accuracy: 0.9967 - val_loss: 0.5278 - val_accuracy: 0.9243\n",
            "Learning rate:  1e-05\n",
            "Epoch 131/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1509 - accuracy: 0.9972 - val_loss: 0.5282 - val_accuracy: 0.9238\n",
            "Learning rate:  1e-05\n",
            "Epoch 132/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1498 - accuracy: 0.9976 - val_loss: 0.5294 - val_accuracy: 0.9242\n",
            "Learning rate:  1e-05\n",
            "Epoch 133/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1502 - accuracy: 0.9968 - val_loss: 0.5316 - val_accuracy: 0.9234\n",
            "Learning rate:  1e-05\n",
            "Epoch 134/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1507 - accuracy: 0.9970 - val_loss: 0.5371 - val_accuracy: 0.9232\n",
            "Learning rate:  1e-05\n",
            "Epoch 135/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1500 - accuracy: 0.9973 - val_loss: 0.5346 - val_accuracy: 0.9240\n",
            "Learning rate:  1e-05\n",
            "Epoch 136/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1496 - accuracy: 0.9974 - val_loss: 0.5348 - val_accuracy: 0.9240\n",
            "Learning rate:  1e-05\n",
            "Epoch 137/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1485 - accuracy: 0.9976 - val_loss: 0.5345 - val_accuracy: 0.9236\n",
            "Learning rate:  1e-05\n",
            "Epoch 138/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1485 - accuracy: 0.9976 - val_loss: 0.5389 - val_accuracy: 0.9232\n",
            "Learning rate:  1e-05\n",
            "Epoch 139/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1492 - accuracy: 0.9973 - val_loss: 0.5376 - val_accuracy: 0.9228\n",
            "Learning rate:  1e-05\n",
            "Epoch 140/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1478 - accuracy: 0.9976 - val_loss: 0.5400 - val_accuracy: 0.9227\n",
            "Learning rate:  1e-05\n",
            "Epoch 141/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1471 - accuracy: 0.9981 - val_loss: 0.5428 - val_accuracy: 0.9229\n",
            "Learning rate:  1e-05\n",
            "Epoch 142/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1477 - accuracy: 0.9976 - val_loss: 0.5401 - val_accuracy: 0.9230\n",
            "Learning rate:  1e-05\n",
            "Epoch 143/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1477 - accuracy: 0.9976 - val_loss: 0.5391 - val_accuracy: 0.9244\n",
            "Learning rate:  1e-05\n",
            "Epoch 144/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1466 - accuracy: 0.9979 - val_loss: 0.5424 - val_accuracy: 0.9231\n",
            "Learning rate:  1e-05\n",
            "Epoch 145/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1474 - accuracy: 0.9972 - val_loss: 0.5442 - val_accuracy: 0.9233\n",
            "Learning rate:  1e-05\n",
            "Epoch 146/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1456 - accuracy: 0.9979 - val_loss: 0.5417 - val_accuracy: 0.9235\n",
            "Learning rate:  1e-05\n",
            "Epoch 147/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1459 - accuracy: 0.9980 - val_loss: 0.5444 - val_accuracy: 0.9228\n",
            "Learning rate:  1e-05\n",
            "Epoch 148/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1456 - accuracy: 0.9981 - val_loss: 0.5411 - val_accuracy: 0.9237\n",
            "Learning rate:  1e-05\n",
            "Epoch 149/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1451 - accuracy: 0.9979 - val_loss: 0.5457 - val_accuracy: 0.9227\n",
            "Learning rate:  1e-05\n",
            "Epoch 150/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1448 - accuracy: 0.9980 - val_loss: 0.5452 - val_accuracy: 0.9241\n",
            "Learning rate:  1e-05\n",
            "Epoch 151/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1448 - accuracy: 0.9979 - val_loss: 0.5476 - val_accuracy: 0.9230\n",
            "Learning rate:  1e-05\n",
            "Epoch 152/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1446 - accuracy: 0.9981 - val_loss: 0.5470 - val_accuracy: 0.9237\n",
            "Learning rate:  1e-05\n",
            "Epoch 153/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1443 - accuracy: 0.9979 - val_loss: 0.5497 - val_accuracy: 0.9239\n",
            "Learning rate:  1e-05\n",
            "Epoch 154/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1443 - accuracy: 0.9979 - val_loss: 0.5481 - val_accuracy: 0.9223\n",
            "Learning rate:  1e-05\n",
            "Epoch 155/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1436 - accuracy: 0.9981 - val_loss: 0.5477 - val_accuracy: 0.9239\n",
            "Learning rate:  1e-05\n",
            "Epoch 156/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1441 - accuracy: 0.9977 - val_loss: 0.5494 - val_accuracy: 0.9223\n",
            "Learning rate:  1e-05\n",
            "Epoch 157/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1436 - accuracy: 0.9979 - val_loss: 0.5506 - val_accuracy: 0.9224\n",
            "Learning rate:  1e-05\n",
            "Epoch 158/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1431 - accuracy: 0.9982 - val_loss: 0.5477 - val_accuracy: 0.9237\n",
            "Learning rate:  1e-05\n",
            "Epoch 159/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1425 - accuracy: 0.9983 - val_loss: 0.5477 - val_accuracy: 0.9239\n",
            "Learning rate:  1e-05\n",
            "Epoch 160/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1428 - accuracy: 0.9981 - val_loss: 0.5475 - val_accuracy: 0.9248\n",
            "Learning rate:  1e-05\n",
            "Epoch 161/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1431 - accuracy: 0.9978 - val_loss: 0.5476 - val_accuracy: 0.9232\n",
            "Learning rate:  1e-06\n",
            "Epoch 162/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1419 - accuracy: 0.9982 - val_loss: 0.5480 - val_accuracy: 0.9231\n",
            "Learning rate:  1e-06\n",
            "Epoch 163/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1420 - accuracy: 0.9981 - val_loss: 0.5491 - val_accuracy: 0.9232\n",
            "Learning rate:  1e-06\n",
            "Epoch 164/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1419 - accuracy: 0.9982 - val_loss: 0.5472 - val_accuracy: 0.9234\n",
            "Learning rate:  1e-06\n",
            "Epoch 165/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1426 - accuracy: 0.9978 - val_loss: 0.5471 - val_accuracy: 0.9234\n",
            "Learning rate:  1e-06\n",
            "Epoch 166/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1412 - accuracy: 0.9986 - val_loss: 0.5466 - val_accuracy: 0.9240\n",
            "Learning rate:  1e-06\n",
            "Epoch 167/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1417 - accuracy: 0.9981 - val_loss: 0.5465 - val_accuracy: 0.9235\n",
            "Learning rate:  1e-06\n",
            "Epoch 168/200\n",
            "390/390 [==============================] - 34s 88ms/step - loss: 0.1415 - accuracy: 0.9984 - val_loss: 0.5475 - val_accuracy: 0.9237\n",
            "Learning rate:  1e-06\n",
            "Epoch 169/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1424 - accuracy: 0.9977 - val_loss: 0.5482 - val_accuracy: 0.9237\n",
            "Learning rate:  1e-06\n",
            "Epoch 170/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1415 - accuracy: 0.9984 - val_loss: 0.5490 - val_accuracy: 0.9234\n",
            "Learning rate:  1e-06\n",
            "Epoch 171/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1417 - accuracy: 0.9981 - val_loss: 0.5484 - val_accuracy: 0.9234\n",
            "Learning rate:  1e-06\n",
            "Epoch 172/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1420 - accuracy: 0.9980 - val_loss: 0.5469 - val_accuracy: 0.9236\n",
            "Learning rate:  1e-06\n",
            "Epoch 173/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1417 - accuracy: 0.9982 - val_loss: 0.5489 - val_accuracy: 0.9240\n",
            "Learning rate:  1e-06\n",
            "Epoch 174/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1414 - accuracy: 0.9983 - val_loss: 0.5469 - val_accuracy: 0.9241\n",
            "Learning rate:  1e-06\n",
            "Epoch 175/200\n",
            "390/390 [==============================] - 36s 92ms/step - loss: 0.1408 - accuracy: 0.9985 - val_loss: 0.5482 - val_accuracy: 0.9235\n",
            "Learning rate:  1e-06\n",
            "Epoch 176/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1413 - accuracy: 0.9983 - val_loss: 0.5484 - val_accuracy: 0.9235\n",
            "Learning rate:  1e-06\n",
            "Epoch 177/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1408 - accuracy: 0.9984 - val_loss: 0.5475 - val_accuracy: 0.9236\n",
            "Learning rate:  1e-06\n",
            "Epoch 178/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1415 - accuracy: 0.9983 - val_loss: 0.5477 - val_accuracy: 0.9240\n",
            "Learning rate:  1e-06\n",
            "Epoch 179/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1410 - accuracy: 0.9984 - val_loss: 0.5490 - val_accuracy: 0.9236\n",
            "Learning rate:  1e-06\n",
            "Epoch 180/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1411 - accuracy: 0.9985 - val_loss: 0.5486 - val_accuracy: 0.9239\n",
            "Learning rate:  1e-06\n",
            "Epoch 181/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1409 - accuracy: 0.9986 - val_loss: 0.5481 - val_accuracy: 0.9237\n",
            "Learning rate:  5e-07\n",
            "Epoch 182/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1412 - accuracy: 0.9983 - val_loss: 0.5475 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 183/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1409 - accuracy: 0.9984 - val_loss: 0.5484 - val_accuracy: 0.9238\n",
            "Learning rate:  5e-07\n",
            "Epoch 184/200\n",
            "390/390 [==============================] - 36s 92ms/step - loss: 0.1410 - accuracy: 0.9982 - val_loss: 0.5480 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 185/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1408 - accuracy: 0.9984 - val_loss: 0.5481 - val_accuracy: 0.9236\n",
            "Learning rate:  5e-07\n",
            "Epoch 186/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1417 - accuracy: 0.9983 - val_loss: 0.5481 - val_accuracy: 0.9238\n",
            "Learning rate:  5e-07\n",
            "Epoch 187/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1406 - accuracy: 0.9986 - val_loss: 0.5477 - val_accuracy: 0.9244\n",
            "Learning rate:  5e-07\n",
            "Epoch 188/200\n",
            "390/390 [==============================] - 35s 89ms/step - loss: 0.1411 - accuracy: 0.9985 - val_loss: 0.5479 - val_accuracy: 0.9245\n",
            "Learning rate:  5e-07\n",
            "Epoch 189/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1409 - accuracy: 0.9983 - val_loss: 0.5480 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 190/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1410 - accuracy: 0.9983 - val_loss: 0.5479 - val_accuracy: 0.9237\n",
            "Learning rate:  5e-07\n",
            "Epoch 191/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1412 - accuracy: 0.9985 - val_loss: 0.5499 - val_accuracy: 0.9236\n",
            "Learning rate:  5e-07\n",
            "Epoch 192/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1404 - accuracy: 0.9986 - val_loss: 0.5479 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 193/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1411 - accuracy: 0.9982 - val_loss: 0.5486 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 194/200\n",
            "390/390 [==============================] - 35s 91ms/step - loss: 0.1403 - accuracy: 0.9986 - val_loss: 0.5486 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 195/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1410 - accuracy: 0.9984 - val_loss: 0.5488 - val_accuracy: 0.9240\n",
            "Learning rate:  5e-07\n",
            "Epoch 196/200\n",
            "390/390 [==============================] - 36s 92ms/step - loss: 0.1410 - accuracy: 0.9985 - val_loss: 0.5488 - val_accuracy: 0.9238\n",
            "Learning rate:  5e-07\n",
            "Epoch 197/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1411 - accuracy: 0.9982 - val_loss: 0.5493 - val_accuracy: 0.9241\n",
            "Learning rate:  5e-07\n",
            "Epoch 198/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1408 - accuracy: 0.9983 - val_loss: 0.5481 - val_accuracy: 0.9238\n",
            "Learning rate:  5e-07\n",
            "Epoch 199/200\n",
            "390/390 [==============================] - 35s 90ms/step - loss: 0.1410 - accuracy: 0.9984 - val_loss: 0.5499 - val_accuracy: 0.9239\n",
            "Learning rate:  5e-07\n",
            "Epoch 200/200\n",
            "390/390 [==============================] - 36s 91ms/step - loss: 0.1413 - accuracy: 0.9982 - val_loss: 0.5495 - val_accuracy: 0.9238\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.5495 - accuracy: 0.9238\n",
            "Test loss: 0.549518346786499\n",
            "Test accuracy: 0.923799991607666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkSn5RPmsfjP",
        "colab_type": "text"
      },
      "source": [
        "## 自己嘗試用 Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyeQXXmZse-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar10\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcnA1uf5t3o3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73aa7267-06c7-4ef6-bddc-2cf37c9dc459"
      },
      "source": [
        "#一般參數設定\n",
        "BATCH_SIZE = 128  \n",
        "epochs = 200\n",
        "num_classes = 10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net = ResNet50(include_top=False, weights='imagenet', input_tensor=None,\n",
        "               input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]))\n",
        "\n",
        "x = net.output\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(num_classes, activation='softmax', name='softmax')(x)\n",
        "\n",
        "net_final = Model(inputs=net.input, outputs=output_layer)\n",
        "\n",
        "#資料\n",
        "train_datagen = ImageDataGenerator(rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   channel_shift_range=10,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "train_batches = train_datagen.flow(x_train,y_train,\n",
        "                                   shuffle=True,batch_size = BATCH_SIZE)\n",
        "\n",
        "# 設定凍結與要進行訓練的網路層 (這邊先不凍結)\n",
        "# FREEZE_LAYERS = 2 \n",
        "# for layer in net_final.layers[:FREEZE_LAYERS]:\n",
        "#     layer.trainable = False\n",
        "# for layer in net_final.layers[FREEZE_LAYERS:]:\n",
        "#     layer.trainable = True\n",
        "\n",
        "\n",
        "net_final.compile(optimizer=Adam(lr=1e-5),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 輸出整個網路結構\n",
        "print(net_final.summary())\n",
        "\n",
        "# 訓練模型\n",
        "net_final.fit_generator(train_batches,\n",
        "                        steps_per_epoch=int(len(x_train)//BATCH_SIZE),\n",
        "                        validation_data=(x_test, y_test),epochs=epochs)\n",
        "\n",
        "# 評估我們的模型\n",
        "scores = net_final.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Dense)                 (None, 10)           20490       dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,608,202\n",
            "Trainable params: 23,555,082\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From <ipython-input-2-d2550baf1586>:57: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/200\n",
            "390/390 [==============================] - 74s 190ms/step - loss: 2.9310 - accuracy: 0.1077 - val_loss: 3.3631 - val_accuracy: 0.0960\n",
            "Epoch 2/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.6253 - accuracy: 0.1212 - val_loss: 3.1097 - val_accuracy: 0.1299\n",
            "Epoch 3/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.4907 - accuracy: 0.1236 - val_loss: 3.2061 - val_accuracy: 0.1514\n",
            "Epoch 4/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.4151 - accuracy: 0.1280 - val_loss: 4.0390 - val_accuracy: 0.1736\n",
            "Epoch 5/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.3746 - accuracy: 0.1285 - val_loss: 4.0444 - val_accuracy: 0.1876\n",
            "Epoch 6/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.3368 - accuracy: 0.1316 - val_loss: 3.9411 - val_accuracy: 0.1961\n",
            "Epoch 7/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.3163 - accuracy: 0.1324 - val_loss: 2.9493 - val_accuracy: 0.1977\n",
            "Epoch 8/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.3019 - accuracy: 0.1307 - val_loss: 2.6537 - val_accuracy: 0.2192\n",
            "Epoch 9/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2896 - accuracy: 0.1342 - val_loss: 2.6117 - val_accuracy: 0.2214\n",
            "Epoch 10/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2819 - accuracy: 0.1347 - val_loss: 2.8276 - val_accuracy: 0.2306\n",
            "Epoch 11/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2779 - accuracy: 0.1352 - val_loss: 2.4510 - val_accuracy: 0.2464\n",
            "Epoch 12/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2708 - accuracy: 0.1374 - val_loss: 2.5721 - val_accuracy: 0.2500\n",
            "Epoch 13/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2690 - accuracy: 0.1389 - val_loss: 2.2020 - val_accuracy: 0.2676\n",
            "Epoch 14/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2655 - accuracy: 0.1387 - val_loss: 2.2734 - val_accuracy: 0.2889\n",
            "Epoch 15/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.2640 - accuracy: 0.1385 - val_loss: 2.2454 - val_accuracy: 0.2937\n",
            "Epoch 16/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2626 - accuracy: 0.1426 - val_loss: 2.1159 - val_accuracy: 0.3022\n",
            "Epoch 17/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2599 - accuracy: 0.1417 - val_loss: 2.0398 - val_accuracy: 0.3181\n",
            "Epoch 18/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2578 - accuracy: 0.1421 - val_loss: 1.9640 - val_accuracy: 0.3328\n",
            "Epoch 19/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2569 - accuracy: 0.1442 - val_loss: 1.9791 - val_accuracy: 0.3337\n",
            "Epoch 20/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2560 - accuracy: 0.1453 - val_loss: 1.9255 - val_accuracy: 0.3321\n",
            "Epoch 21/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2555 - accuracy: 0.1423 - val_loss: 2.1427 - val_accuracy: 0.3402\n",
            "Epoch 22/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2539 - accuracy: 0.1451 - val_loss: 1.9544 - val_accuracy: 0.3439\n",
            "Epoch 23/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2496 - accuracy: 0.1453 - val_loss: 1.7892 - val_accuracy: 0.3483\n",
            "Epoch 24/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2504 - accuracy: 0.1460 - val_loss: 1.7862 - val_accuracy: 0.3529\n",
            "Epoch 25/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2484 - accuracy: 0.1459 - val_loss: 1.7480 - val_accuracy: 0.3645\n",
            "Epoch 26/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2488 - accuracy: 0.1448 - val_loss: 1.7770 - val_accuracy: 0.3743\n",
            "Epoch 27/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2444 - accuracy: 0.1462 - val_loss: 1.8726 - val_accuracy: 0.3807\n",
            "Epoch 28/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2415 - accuracy: 0.1508 - val_loss: 1.7752 - val_accuracy: 0.4046\n",
            "Epoch 29/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2399 - accuracy: 0.1527 - val_loss: 1.8497 - val_accuracy: 0.3985\n",
            "Epoch 30/200\n",
            "390/390 [==============================] - 70s 180ms/step - loss: 2.2419 - accuracy: 0.1478 - val_loss: 1.6939 - val_accuracy: 0.4180\n",
            "Epoch 31/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2410 - accuracy: 0.1490 - val_loss: 1.6614 - val_accuracy: 0.4281\n",
            "Epoch 32/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2386 - accuracy: 0.1527 - val_loss: 1.7035 - val_accuracy: 0.4395\n",
            "Epoch 33/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.2366 - accuracy: 0.1521 - val_loss: 1.5470 - val_accuracy: 0.4586\n",
            "Epoch 34/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2368 - accuracy: 0.1536 - val_loss: 1.6938 - val_accuracy: 0.4601\n",
            "Epoch 35/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2344 - accuracy: 0.1534 - val_loss: 1.6419 - val_accuracy: 0.4609\n",
            "Epoch 36/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2308 - accuracy: 0.1555 - val_loss: 1.5205 - val_accuracy: 0.4764\n",
            "Epoch 37/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2313 - accuracy: 0.1526 - val_loss: 1.5172 - val_accuracy: 0.4676\n",
            "Epoch 38/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2328 - accuracy: 0.1525 - val_loss: 1.5252 - val_accuracy: 0.4791\n",
            "Epoch 39/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2305 - accuracy: 0.1559 - val_loss: 1.5447 - val_accuracy: 0.4950\n",
            "Epoch 40/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2298 - accuracy: 0.1578 - val_loss: 1.7927 - val_accuracy: 0.4875\n",
            "Epoch 41/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2261 - accuracy: 0.1556 - val_loss: 1.5479 - val_accuracy: 0.4858\n",
            "Epoch 42/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2265 - accuracy: 0.1568 - val_loss: 1.3679 - val_accuracy: 0.5292\n",
            "Epoch 43/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2238 - accuracy: 0.1573 - val_loss: 1.5861 - val_accuracy: 0.4991\n",
            "Epoch 44/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2244 - accuracy: 0.1590 - val_loss: 1.4577 - val_accuracy: 0.5108\n",
            "Epoch 45/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2227 - accuracy: 0.1603 - val_loss: 1.9086 - val_accuracy: 0.4934\n",
            "Epoch 46/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2224 - accuracy: 0.1607 - val_loss: 1.4074 - val_accuracy: 0.5351\n",
            "Epoch 47/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2214 - accuracy: 0.1611 - val_loss: 1.3939 - val_accuracy: 0.5301\n",
            "Epoch 48/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2198 - accuracy: 0.1608 - val_loss: 1.4667 - val_accuracy: 0.5227\n",
            "Epoch 49/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2166 - accuracy: 0.1625 - val_loss: 1.3782 - val_accuracy: 0.5444\n",
            "Epoch 50/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2176 - accuracy: 0.1598 - val_loss: 1.4992 - val_accuracy: 0.5326\n",
            "Epoch 51/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2282 - accuracy: 0.1585 - val_loss: 1.3554 - val_accuracy: 0.5476\n",
            "Epoch 52/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2196 - accuracy: 0.1605 - val_loss: 1.3014 - val_accuracy: 0.5501\n",
            "Epoch 53/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2152 - accuracy: 0.1630 - val_loss: 1.2907 - val_accuracy: 0.5544\n",
            "Epoch 54/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2154 - accuracy: 0.1641 - val_loss: 1.3863 - val_accuracy: 0.5546\n",
            "Epoch 55/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2136 - accuracy: 0.1646 - val_loss: 1.3882 - val_accuracy: 0.5433\n",
            "Epoch 56/200\n",
            "390/390 [==============================] - 70s 180ms/step - loss: 2.2162 - accuracy: 0.1657 - val_loss: 1.4215 - val_accuracy: 0.5646\n",
            "Epoch 57/200\n",
            "390/390 [==============================] - 70s 181ms/step - loss: 2.2107 - accuracy: 0.1656 - val_loss: 1.3360 - val_accuracy: 0.5531\n",
            "Epoch 58/200\n",
            "390/390 [==============================] - 70s 181ms/step - loss: 2.2154 - accuracy: 0.1611 - val_loss: 1.3514 - val_accuracy: 0.5639\n",
            "Epoch 59/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.2144 - accuracy: 0.1600 - val_loss: 1.3692 - val_accuracy: 0.5432\n",
            "Epoch 60/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2121 - accuracy: 0.1652 - val_loss: 1.2760 - val_accuracy: 0.5525\n",
            "Epoch 61/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2151 - accuracy: 0.1625 - val_loss: 1.2854 - val_accuracy: 0.5742\n",
            "Epoch 62/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2135 - accuracy: 0.1615 - val_loss: 1.4106 - val_accuracy: 0.5733\n",
            "Epoch 63/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2129 - accuracy: 0.1638 - val_loss: 1.2924 - val_accuracy: 0.5816\n",
            "Epoch 64/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.2101 - accuracy: 0.1638 - val_loss: 1.3506 - val_accuracy: 0.5642\n",
            "Epoch 65/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2106 - accuracy: 0.1641 - val_loss: 1.2974 - val_accuracy: 0.5694\n",
            "Epoch 66/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2117 - accuracy: 0.1641 - val_loss: 1.2336 - val_accuracy: 0.5841\n",
            "Epoch 67/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2134 - accuracy: 0.1664 - val_loss: 1.3267 - val_accuracy: 0.5745\n",
            "Epoch 68/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.2108 - accuracy: 0.1653 - val_loss: 1.2375 - val_accuracy: 0.5839\n",
            "Epoch 69/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.2067 - accuracy: 0.1686 - val_loss: 1.2706 - val_accuracy: 0.5917\n",
            "Epoch 70/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2091 - accuracy: 0.1657 - val_loss: 1.2117 - val_accuracy: 0.6077\n",
            "Epoch 71/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2076 - accuracy: 0.1655 - val_loss: 1.7960 - val_accuracy: 0.5863\n",
            "Epoch 72/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.2074 - accuracy: 0.1673 - val_loss: 1.2891 - val_accuracy: 0.5724\n",
            "Epoch 73/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2089 - accuracy: 0.1642 - val_loss: 1.4091 - val_accuracy: 0.5690\n",
            "Epoch 74/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2051 - accuracy: 0.1688 - val_loss: 1.3185 - val_accuracy: 0.6104\n",
            "Epoch 75/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2036 - accuracy: 0.1669 - val_loss: 1.3333 - val_accuracy: 0.6091\n",
            "Epoch 76/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2056 - accuracy: 0.1665 - val_loss: 1.2287 - val_accuracy: 0.6111\n",
            "Epoch 77/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2076 - accuracy: 0.1681 - val_loss: 1.0954 - val_accuracy: 0.6206\n",
            "Epoch 78/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2077 - accuracy: 0.1670 - val_loss: 1.0720 - val_accuracy: 0.6330\n",
            "Epoch 79/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2035 - accuracy: 0.1677 - val_loss: 1.1916 - val_accuracy: 0.6072\n",
            "Epoch 80/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2023 - accuracy: 0.1689 - val_loss: 1.1519 - val_accuracy: 0.6107\n",
            "Epoch 81/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1997 - accuracy: 0.1727 - val_loss: 1.2151 - val_accuracy: 0.6228\n",
            "Epoch 82/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.2032 - accuracy: 0.1676 - val_loss: 1.2081 - val_accuracy: 0.6191\n",
            "Epoch 83/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2042 - accuracy: 0.1665 - val_loss: 1.2710 - val_accuracy: 0.6096\n",
            "Epoch 84/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2045 - accuracy: 0.1661 - val_loss: 1.3025 - val_accuracy: 0.5990\n",
            "Epoch 85/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2001 - accuracy: 0.1673 - val_loss: 1.1481 - val_accuracy: 0.6232\n",
            "Epoch 86/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2030 - accuracy: 0.1655 - val_loss: 1.1467 - val_accuracy: 0.6214\n",
            "Epoch 87/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1991 - accuracy: 0.1678 - val_loss: 1.1176 - val_accuracy: 0.6276\n",
            "Epoch 88/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.2040 - accuracy: 0.1674 - val_loss: 1.1301 - val_accuracy: 0.6376\n",
            "Epoch 89/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.1979 - accuracy: 0.1682 - val_loss: 1.1562 - val_accuracy: 0.6299\n",
            "Epoch 90/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.2026 - accuracy: 0.1682 - val_loss: 1.1157 - val_accuracy: 0.6358\n",
            "Epoch 91/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.2011 - accuracy: 0.1688 - val_loss: 1.1057 - val_accuracy: 0.6410\n",
            "Epoch 92/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1995 - accuracy: 0.1710 - val_loss: 1.1597 - val_accuracy: 0.6323\n",
            "Epoch 93/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1959 - accuracy: 0.1718 - val_loss: 1.1433 - val_accuracy: 0.6339\n",
            "Epoch 94/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.2010 - accuracy: 0.1665 - val_loss: 1.1929 - val_accuracy: 0.6260\n",
            "Epoch 95/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.1973 - accuracy: 0.1701 - val_loss: 1.1907 - val_accuracy: 0.6440\n",
            "Epoch 96/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1953 - accuracy: 0.1723 - val_loss: 1.1055 - val_accuracy: 0.6411\n",
            "Epoch 97/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1988 - accuracy: 0.1687 - val_loss: 1.3890 - val_accuracy: 0.6165\n",
            "Epoch 98/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1957 - accuracy: 0.1723 - val_loss: 1.0498 - val_accuracy: 0.6469\n",
            "Epoch 99/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1959 - accuracy: 0.1696 - val_loss: 1.1272 - val_accuracy: 0.6347\n",
            "Epoch 100/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1978 - accuracy: 0.1688 - val_loss: 1.1702 - val_accuracy: 0.6242\n",
            "Epoch 101/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1956 - accuracy: 0.1720 - val_loss: 1.2414 - val_accuracy: 0.6268\n",
            "Epoch 102/200\n",
            "390/390 [==============================] - 70s 180ms/step - loss: 2.1961 - accuracy: 0.1709 - val_loss: 1.1765 - val_accuracy: 0.6345\n",
            "Epoch 103/200\n",
            "390/390 [==============================] - 70s 180ms/step - loss: 2.1954 - accuracy: 0.1700 - val_loss: 1.1300 - val_accuracy: 0.6320\n",
            "Epoch 104/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1963 - accuracy: 0.1717 - val_loss: 1.0954 - val_accuracy: 0.6430\n",
            "Epoch 105/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1946 - accuracy: 0.1711 - val_loss: 1.2602 - val_accuracy: 0.6200\n",
            "Epoch 106/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1949 - accuracy: 0.1743 - val_loss: 1.2336 - val_accuracy: 0.6437\n",
            "Epoch 107/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1931 - accuracy: 0.1724 - val_loss: 0.9915 - val_accuracy: 0.6712\n",
            "Epoch 108/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1925 - accuracy: 0.1714 - val_loss: 1.1502 - val_accuracy: 0.6364\n",
            "Epoch 109/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1924 - accuracy: 0.1704 - val_loss: 1.1156 - val_accuracy: 0.6400\n",
            "Epoch 110/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1915 - accuracy: 0.1730 - val_loss: 1.2076 - val_accuracy: 0.6270\n",
            "Epoch 111/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1931 - accuracy: 0.1720 - val_loss: 1.1871 - val_accuracy: 0.6295\n",
            "Epoch 112/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1945 - accuracy: 0.1723 - val_loss: 0.9622 - val_accuracy: 0.6714\n",
            "Epoch 113/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1940 - accuracy: 0.1716 - val_loss: 1.1373 - val_accuracy: 0.6345\n",
            "Epoch 114/200\n",
            "390/390 [==============================] - 70s 180ms/step - loss: 2.1902 - accuracy: 0.1734 - val_loss: 1.0114 - val_accuracy: 0.6618\n",
            "Epoch 115/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1927 - accuracy: 0.1722 - val_loss: 1.2532 - val_accuracy: 0.6382\n",
            "Epoch 116/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1910 - accuracy: 0.1740 - val_loss: 1.0638 - val_accuracy: 0.6633\n",
            "Epoch 117/200\n",
            "390/390 [==============================] - 70s 181ms/step - loss: 2.1932 - accuracy: 0.1705 - val_loss: 1.1255 - val_accuracy: 0.6484\n",
            "Epoch 118/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1881 - accuracy: 0.1742 - val_loss: 1.2188 - val_accuracy: 0.6337\n",
            "Epoch 119/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1918 - accuracy: 0.1694 - val_loss: 0.9676 - val_accuracy: 0.6784\n",
            "Epoch 120/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.1909 - accuracy: 0.1720 - val_loss: 1.4925 - val_accuracy: 0.6483\n",
            "Epoch 121/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1921 - accuracy: 0.1695 - val_loss: 1.1128 - val_accuracy: 0.6441\n",
            "Epoch 122/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1894 - accuracy: 0.1730 - val_loss: 1.0519 - val_accuracy: 0.6642\n",
            "Epoch 123/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1906 - accuracy: 0.1737 - val_loss: 1.0376 - val_accuracy: 0.6620\n",
            "Epoch 124/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1940 - accuracy: 0.1738 - val_loss: 1.1996 - val_accuracy: 0.6541\n",
            "Epoch 125/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.1901 - accuracy: 0.1719 - val_loss: 1.0887 - val_accuracy: 0.6623\n",
            "Epoch 126/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1876 - accuracy: 0.1748 - val_loss: 1.1533 - val_accuracy: 0.6603\n",
            "Epoch 127/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.1890 - accuracy: 0.1734 - val_loss: 1.0911 - val_accuracy: 0.6650\n",
            "Epoch 128/200\n",
            "390/390 [==============================] - 72s 186ms/step - loss: 2.1884 - accuracy: 0.1750 - val_loss: 0.9982 - val_accuracy: 0.6619\n",
            "Epoch 129/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1896 - accuracy: 0.1740 - val_loss: 0.9841 - val_accuracy: 0.6757\n",
            "Epoch 130/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1886 - accuracy: 0.1738 - val_loss: 0.9727 - val_accuracy: 0.6844\n",
            "Epoch 131/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1853 - accuracy: 0.1756 - val_loss: 1.0531 - val_accuracy: 0.6772\n",
            "Epoch 132/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1852 - accuracy: 0.1757 - val_loss: 0.9445 - val_accuracy: 0.6828\n",
            "Epoch 133/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1863 - accuracy: 0.1724 - val_loss: 1.0041 - val_accuracy: 0.6723\n",
            "Epoch 134/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1896 - accuracy: 0.1724 - val_loss: 0.9931 - val_accuracy: 0.6710\n",
            "Epoch 135/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1877 - accuracy: 0.1758 - val_loss: 1.0685 - val_accuracy: 0.6610\n",
            "Epoch 136/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1827 - accuracy: 0.1755 - val_loss: 1.1311 - val_accuracy: 0.6482\n",
            "Epoch 137/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1850 - accuracy: 0.1752 - val_loss: 1.0967 - val_accuracy: 0.6599\n",
            "Epoch 138/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1851 - accuracy: 0.1773 - val_loss: 0.9580 - val_accuracy: 0.6816\n",
            "Epoch 139/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1860 - accuracy: 0.1763 - val_loss: 0.9836 - val_accuracy: 0.6835\n",
            "Epoch 140/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1859 - accuracy: 0.1763 - val_loss: 1.0992 - val_accuracy: 0.6750\n",
            "Epoch 141/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1857 - accuracy: 0.1761 - val_loss: 1.3417 - val_accuracy: 0.6682\n",
            "Epoch 142/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1863 - accuracy: 0.1744 - val_loss: 0.9005 - val_accuracy: 0.6990\n",
            "Epoch 143/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1848 - accuracy: 0.1759 - val_loss: 1.0577 - val_accuracy: 0.6748\n",
            "Epoch 144/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1865 - accuracy: 0.1755 - val_loss: 0.9729 - val_accuracy: 0.6904\n",
            "Epoch 145/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1845 - accuracy: 0.1757 - val_loss: 1.2345 - val_accuracy: 0.6607\n",
            "Epoch 146/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1824 - accuracy: 0.1755 - val_loss: 1.6186 - val_accuracy: 0.6393\n",
            "Epoch 147/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1854 - accuracy: 0.1753 - val_loss: 1.2466 - val_accuracy: 0.6702\n",
            "Epoch 148/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1818 - accuracy: 0.1753 - val_loss: 1.1070 - val_accuracy: 0.6742\n",
            "Epoch 149/200\n",
            "390/390 [==============================] - 70s 180ms/step - loss: 2.1872 - accuracy: 0.1736 - val_loss: 1.0253 - val_accuracy: 0.6897\n",
            "Epoch 150/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1838 - accuracy: 0.1768 - val_loss: 1.1204 - val_accuracy: 0.6590\n",
            "Epoch 151/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1851 - accuracy: 0.1727 - val_loss: 1.0221 - val_accuracy: 0.6877\n",
            "Epoch 152/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1857 - accuracy: 0.1773 - val_loss: 1.4927 - val_accuracy: 0.6578\n",
            "Epoch 153/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1806 - accuracy: 0.1790 - val_loss: 1.0619 - val_accuracy: 0.6801\n",
            "Epoch 154/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1807 - accuracy: 0.1777 - val_loss: 1.1164 - val_accuracy: 0.6912\n",
            "Epoch 155/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1845 - accuracy: 0.1761 - val_loss: 1.2818 - val_accuracy: 0.6784\n",
            "Epoch 156/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1830 - accuracy: 0.1753 - val_loss: 1.0645 - val_accuracy: 0.6818\n",
            "Epoch 157/200\n",
            "390/390 [==============================] - 72s 183ms/step - loss: 2.1831 - accuracy: 0.1754 - val_loss: 0.9802 - val_accuracy: 0.6998\n",
            "Epoch 158/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1859 - accuracy: 0.1752 - val_loss: 0.9995 - val_accuracy: 0.6892\n",
            "Epoch 159/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1822 - accuracy: 0.1758 - val_loss: 1.0196 - val_accuracy: 0.6938\n",
            "Epoch 160/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1842 - accuracy: 0.1752 - val_loss: 1.0910 - val_accuracy: 0.6669\n",
            "Epoch 161/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1822 - accuracy: 0.1779 - val_loss: 1.0380 - val_accuracy: 0.6862\n",
            "Epoch 162/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1830 - accuracy: 0.1718 - val_loss: 1.0398 - val_accuracy: 0.6906\n",
            "Epoch 163/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1809 - accuracy: 0.1753 - val_loss: 0.9985 - val_accuracy: 0.6817\n",
            "Epoch 164/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1802 - accuracy: 0.1767 - val_loss: 1.0531 - val_accuracy: 0.6895\n",
            "Epoch 165/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1791 - accuracy: 0.1772 - val_loss: 1.1936 - val_accuracy: 0.6869\n",
            "Epoch 166/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1854 - accuracy: 0.1756 - val_loss: 0.9892 - val_accuracy: 0.6865\n",
            "Epoch 167/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1819 - accuracy: 0.1775 - val_loss: 1.0804 - val_accuracy: 0.6694\n",
            "Epoch 168/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1832 - accuracy: 0.1768 - val_loss: 0.9653 - val_accuracy: 0.7004\n",
            "Epoch 169/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1806 - accuracy: 0.1769 - val_loss: 1.1464 - val_accuracy: 0.6836\n",
            "Epoch 170/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1822 - accuracy: 0.1755 - val_loss: 1.2266 - val_accuracy: 0.6766\n",
            "Epoch 171/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1800 - accuracy: 0.1764 - val_loss: 1.2209 - val_accuracy: 0.6844\n",
            "Epoch 172/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1815 - accuracy: 0.1766 - val_loss: 1.0366 - val_accuracy: 0.6910\n",
            "Epoch 173/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1843 - accuracy: 0.1738 - val_loss: 1.3035 - val_accuracy: 0.6740\n",
            "Epoch 174/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1854 - accuracy: 0.1760 - val_loss: 0.9711 - val_accuracy: 0.6931\n",
            "Epoch 175/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1851 - accuracy: 0.1771 - val_loss: 0.9043 - val_accuracy: 0.7072\n",
            "Epoch 176/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1792 - accuracy: 0.1772 - val_loss: 1.2052 - val_accuracy: 0.6632\n",
            "Epoch 177/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1791 - accuracy: 0.1780 - val_loss: 1.1022 - val_accuracy: 0.6719\n",
            "Epoch 178/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1825 - accuracy: 0.1764 - val_loss: 0.9077 - val_accuracy: 0.7066\n",
            "Epoch 179/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1785 - accuracy: 0.1802 - val_loss: 1.1057 - val_accuracy: 0.6802\n",
            "Epoch 180/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1812 - accuracy: 0.1740 - val_loss: 0.9635 - val_accuracy: 0.7014\n",
            "Epoch 181/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1774 - accuracy: 0.1798 - val_loss: 1.1118 - val_accuracy: 0.6927\n",
            "Epoch 182/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1794 - accuracy: 0.1787 - val_loss: 1.4984 - val_accuracy: 0.6581\n",
            "Epoch 183/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1801 - accuracy: 0.1770 - val_loss: 1.5860 - val_accuracy: 0.6762\n",
            "Epoch 184/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1801 - accuracy: 0.1781 - val_loss: 1.0124 - val_accuracy: 0.7081\n",
            "Epoch 185/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1812 - accuracy: 0.1782 - val_loss: 1.2583 - val_accuracy: 0.6792\n",
            "Epoch 186/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1794 - accuracy: 0.1782 - val_loss: 1.8108 - val_accuracy: 0.6716\n",
            "Epoch 187/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1781 - accuracy: 0.1767 - val_loss: 1.4031 - val_accuracy: 0.6827\n",
            "Epoch 188/200\n",
            "390/390 [==============================] - 71s 181ms/step - loss: 2.1803 - accuracy: 0.1753 - val_loss: 1.1564 - val_accuracy: 0.6881\n",
            "Epoch 189/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1773 - accuracy: 0.1777 - val_loss: 1.4689 - val_accuracy: 0.6880\n",
            "Epoch 190/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1745 - accuracy: 0.1807 - val_loss: 1.0056 - val_accuracy: 0.6978\n",
            "Epoch 191/200\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.1790 - accuracy: 0.1760 - val_loss: 1.0031 - val_accuracy: 0.7122\n",
            "Epoch 192/200\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.1766 - accuracy: 0.1778 - val_loss: 0.9950 - val_accuracy: 0.7112\n",
            "Epoch 193/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1807 - accuracy: 0.1766 - val_loss: 0.9710 - val_accuracy: 0.7111\n",
            "Epoch 194/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1763 - accuracy: 0.1789 - val_loss: 0.8728 - val_accuracy: 0.7141\n",
            "Epoch 195/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1816 - accuracy: 0.1775 - val_loss: 1.0033 - val_accuracy: 0.6870\n",
            "Epoch 196/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1785 - accuracy: 0.1754 - val_loss: 0.8701 - val_accuracy: 0.7195\n",
            "Epoch 197/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1793 - accuracy: 0.1784 - val_loss: 1.2621 - val_accuracy: 0.6883\n",
            "Epoch 198/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1726 - accuracy: 0.1809 - val_loss: 1.0940 - val_accuracy: 0.7006\n",
            "Epoch 199/200\n",
            "390/390 [==============================] - 72s 185ms/step - loss: 2.1784 - accuracy: 0.1779 - val_loss: 0.8605 - val_accuracy: 0.7255\n",
            "Epoch 200/200\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 2.1804 - accuracy: 0.1762 - val_loss: 1.2292 - val_accuracy: 0.6930\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 1.2292 - accuracy: 0.6930\n",
            "Test loss: 1.2292115688323975\n",
            "Test accuracy: 0.6930000185966492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KpZ8k2bane1",
        "colab_type": "text"
      },
      "source": [
        "使用transfer learning的效果並不好，可能是因為原本像素是較高的256*256\n",
        "而這邊使用的資料集是32*32"
      ]
    }
  ]
}