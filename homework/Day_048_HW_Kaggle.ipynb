{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV,cross_val_score\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('data/data-science-london-scikit-learn/train.csv',header=None)\n",
    "train_y = pd.read_csv('data/data-science-london-scikit-learn/trainLabels.csv',header=None)\n",
    "test_x = pd.read_csv('data/data-science-london-scikit-learn/test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conca = pd.concat([train_x,train_y],axis=1)\n",
    "corr = conca.corr()\n",
    "cor = corr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "# 繪製相關係數 (correlations) 的 Heatmap\n",
    "sns.heatmap(cor, cmap = plt.cm.RdYlBu_r, vmin = -0.25, vmax = 0.6)  #annot選擇是否每格顯示數字\n",
    "plt.title('Correlation Heatmap'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 41 columns):\n",
      "0     1000 non-null float64\n",
      "1     1000 non-null float64\n",
      "2     1000 non-null float64\n",
      "3     1000 non-null float64\n",
      "4     1000 non-null float64\n",
      "5     1000 non-null float64\n",
      "6     1000 non-null float64\n",
      "7     1000 non-null float64\n",
      "8     1000 non-null float64\n",
      "9     1000 non-null float64\n",
      "10    1000 non-null float64\n",
      "11    1000 non-null float64\n",
      "12    1000 non-null float64\n",
      "13    1000 non-null float64\n",
      "14    1000 non-null float64\n",
      "15    1000 non-null float64\n",
      "16    1000 non-null float64\n",
      "17    1000 non-null float64\n",
      "18    1000 non-null float64\n",
      "19    1000 non-null float64\n",
      "20    1000 non-null float64\n",
      "21    1000 non-null float64\n",
      "22    1000 non-null float64\n",
      "23    1000 non-null float64\n",
      "24    1000 non-null float64\n",
      "25    1000 non-null float64\n",
      "26    1000 non-null float64\n",
      "27    1000 non-null float64\n",
      "28    1000 non-null float64\n",
      "29    1000 non-null float64\n",
      "30    1000 non-null float64\n",
      "31    1000 non-null float64\n",
      "32    1000 non-null float64\n",
      "33    1000 non-null float64\n",
      "34    1000 non-null float64\n",
      "35    1000 non-null float64\n",
      "36    1000 non-null float64\n",
      "37    1000 non-null float64\n",
      "38    1000 non-null float64\n",
      "39    1000 non-null float64\n",
      "0     1000 non-null int64\n",
      "dtypes: float64(40), int64(1)\n",
      "memory usage: 320.4 KB\n"
     ]
    }
   ],
   "source": [
    "conca.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.025596</td>\n",
       "      <td>-0.024526</td>\n",
       "      <td>-0.024088</td>\n",
       "      <td>-0.002271</td>\n",
       "      <td>1.092329</td>\n",
       "      <td>-0.006250</td>\n",
       "      <td>0.497342</td>\n",
       "      <td>-0.037883</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022951</td>\n",
       "      <td>-0.542491</td>\n",
       "      <td>-0.011608</td>\n",
       "      <td>-0.483507</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>0.567185</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>-0.892659</td>\n",
       "      <td>0.609451</td>\n",
       "      <td>0.51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.008282</td>\n",
       "      <td>1.016298</td>\n",
       "      <td>0.979109</td>\n",
       "      <td>0.970575</td>\n",
       "      <td>4.538834</td>\n",
       "      <td>0.989128</td>\n",
       "      <td>2.118819</td>\n",
       "      <td>2.232256</td>\n",
       "      <td>1.001064</td>\n",
       "      <td>1.013520</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001375</td>\n",
       "      <td>2.239939</td>\n",
       "      <td>1.022456</td>\n",
       "      <td>2.121281</td>\n",
       "      <td>1.007044</td>\n",
       "      <td>2.227876</td>\n",
       "      <td>0.997635</td>\n",
       "      <td>2.022022</td>\n",
       "      <td>2.045439</td>\n",
       "      <td>0.50015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-3.365711</td>\n",
       "      <td>-3.492086</td>\n",
       "      <td>-2.695602</td>\n",
       "      <td>-3.460471</td>\n",
       "      <td>-16.421901</td>\n",
       "      <td>-3.041250</td>\n",
       "      <td>-7.224761</td>\n",
       "      <td>-6.509084</td>\n",
       "      <td>-3.145588</td>\n",
       "      <td>-2.749812</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.971125</td>\n",
       "      <td>-7.840890</td>\n",
       "      <td>-2.999564</td>\n",
       "      <td>-7.124105</td>\n",
       "      <td>-2.952358</td>\n",
       "      <td>-5.452254</td>\n",
       "      <td>-3.473913</td>\n",
       "      <td>-8.051722</td>\n",
       "      <td>-7.799086</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-0.669010</td>\n",
       "      <td>-0.693937</td>\n",
       "      <td>-0.698830</td>\n",
       "      <td>-0.617557</td>\n",
       "      <td>-1.801997</td>\n",
       "      <td>-0.732265</td>\n",
       "      <td>-0.838619</td>\n",
       "      <td>-1.604037</td>\n",
       "      <td>-0.677562</td>\n",
       "      <td>-0.682220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.696032</td>\n",
       "      <td>-2.121943</td>\n",
       "      <td>-0.664550</td>\n",
       "      <td>-1.879247</td>\n",
       "      <td>-0.642861</td>\n",
       "      <td>-1.059786</td>\n",
       "      <td>-0.691162</td>\n",
       "      <td>-2.220126</td>\n",
       "      <td>-0.565041</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.027895</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.862818</td>\n",
       "      <td>0.027041</td>\n",
       "      <td>0.582321</td>\n",
       "      <td>0.018809</td>\n",
       "      <td>0.022092</td>\n",
       "      <td>-0.036110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049778</td>\n",
       "      <td>-0.568262</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>-0.493575</td>\n",
       "      <td>0.037732</td>\n",
       "      <td>0.455474</td>\n",
       "      <td>0.038284</td>\n",
       "      <td>-0.855470</td>\n",
       "      <td>0.779944</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.762520</td>\n",
       "      <td>0.682753</td>\n",
       "      <td>0.661434</td>\n",
       "      <td>0.640743</td>\n",
       "      <td>3.843172</td>\n",
       "      <td>0.671456</td>\n",
       "      <td>1.913664</td>\n",
       "      <td>1.438304</td>\n",
       "      <td>0.741310</td>\n",
       "      <td>0.665364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699917</td>\n",
       "      <td>0.939348</td>\n",
       "      <td>0.651374</td>\n",
       "      <td>1.005795</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>2.122157</td>\n",
       "      <td>0.693535</td>\n",
       "      <td>0.388698</td>\n",
       "      <td>1.992193</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>3.326246</td>\n",
       "      <td>3.583870</td>\n",
       "      <td>2.546507</td>\n",
       "      <td>3.088738</td>\n",
       "      <td>17.565345</td>\n",
       "      <td>3.102997</td>\n",
       "      <td>7.592666</td>\n",
       "      <td>7.130097</td>\n",
       "      <td>3.145258</td>\n",
       "      <td>3.919426</td>\n",
       "      <td>...</td>\n",
       "      <td>3.688047</td>\n",
       "      <td>7.160379</td>\n",
       "      <td>3.353631</td>\n",
       "      <td>6.005818</td>\n",
       "      <td>3.420561</td>\n",
       "      <td>6.603499</td>\n",
       "      <td>3.492548</td>\n",
       "      <td>5.774120</td>\n",
       "      <td>6.803984</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.025596    -0.024526    -0.024088    -0.002271     1.092329   \n",
       "std       1.008282     1.016298     0.979109     0.970575     4.538834   \n",
       "min      -3.365711    -3.492086    -2.695602    -3.460471   -16.421901   \n",
       "25%      -0.669010    -0.693937    -0.698830    -0.617557    -1.801997   \n",
       "50%       0.027895    -0.033194     0.008145     0.002327     0.862818   \n",
       "75%       0.762520     0.682753     0.661434     0.640743     3.843172   \n",
       "max       3.326246     3.583870     2.546507     3.088738    17.565345   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -0.006250     0.497342    -0.037883     0.026391    -0.003597  ...   \n",
       "std       0.989128     2.118819     2.232256     1.001064     1.013520  ...   \n",
       "min      -3.041250    -7.224761    -6.509084    -3.145588    -2.749812  ...   \n",
       "25%      -0.732265    -0.838619    -1.604037    -0.677562    -0.682220  ...   \n",
       "50%       0.027041     0.582321     0.018809     0.022092    -0.036110  ...   \n",
       "75%       0.671456     1.913664     1.438304     0.741310     0.665364  ...   \n",
       "max       3.102997     7.592666     7.130097     3.145258     3.919426  ...   \n",
       "\n",
       "                31           32           33           34           35  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.022951    -0.542491    -0.011608    -0.483507     0.033371   \n",
       "std       1.001375     2.239939     1.022456     2.121281     1.007044   \n",
       "min      -2.971125    -7.840890    -2.999564    -7.124105    -2.952358   \n",
       "25%      -0.696032    -2.121943    -0.664550    -1.879247    -0.642861   \n",
       "50%       0.049778    -0.568262    -0.028097    -0.493575     0.037732   \n",
       "75%       0.699917     0.939348     0.651374     1.005795     0.691800   \n",
       "max       3.688047     7.160379     3.353631     6.005818     3.420561   \n",
       "\n",
       "                36           37           38           39          0   \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.00000  \n",
       "mean      0.567185     0.006849    -0.892659     0.609451     0.51000  \n",
       "std       2.227876     0.997635     2.022022     2.045439     0.50015  \n",
       "min      -5.452254    -3.473913    -8.051722    -7.799086     0.00000  \n",
       "25%      -1.059786    -0.691162    -2.220126    -0.565041     0.00000  \n",
       "50%       0.455474     0.038284    -0.855470     0.779944     1.00000  \n",
       "75%       2.122157     0.693535     0.388698     1.992193     1.00000  \n",
       "max       6.603499     3.492548     5.774120     6.803984     1.00000  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "std = StandardScaler()\n",
    "X_std = std.fit_transform(train_x)\n",
    "mms = MinMaxScaler()\n",
    "X_mms = mms.fit_transform(train_x)\n",
    "norm = Normalizer()\n",
    "X_norm = norm.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_std, train_y, test_size=0.3,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- train_x ------------------\n",
      "Naive Bayes 0.8233333333333334\n",
      "KNN 0.89\n",
      "Random Forest 0.8533333333333334\n",
      "Logistic Regression 0.84\n",
      "SVM 0.9166666666666666\n",
      "Decision Tree 0.7433333333333333\n",
      "GradientBoosting 0.87\n",
      "ADABOOST 0.8633333333333333\n",
      "---------------- X_std ------------------\n",
      "Naive Bayes 0.8233333333333334\n",
      "KNN 0.8366666666666667\n",
      "Random Forest 0.8533333333333334\n",
      "Logistic Regression 0.84\n",
      "SVM 0.8866666666666667\n",
      "Decision Tree 0.75\n",
      "GradientBoosting 0.87\n",
      "ADABOOST 0.8633333333333333\n",
      "---------------- X_mms ------------------\n",
      "Naive Bayes 0.8233333333333334\n",
      "KNN 0.84\n",
      "Random Forest 0.8533333333333334\n",
      "Logistic Regression 0.8333333333333334\n",
      "SVM 0.8266666666666667\n",
      "Decision Tree 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting 0.87\n",
      "ADABOOST 0.8633333333333333\n",
      "---------------- X_norm ------------------\n",
      "Naive Bayes 0.8033333333333333\n",
      "KNN 0.9033333333333333\n",
      "Random Forest 0.86\n",
      "Logistic Regression 0.8466666666666667\n",
      "SVM 0.8233333333333334\n",
      "Decision Tree 0.7866666666666666\n",
      "GradientBoosting 0.87\n",
      "ADABOOST 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "#來試試看個模型的成效\n",
    "\n",
    "stand = {\"train_x\":train_x,\"X_std\":X_std,\"X_mms\":X_mms,\"X_norm\":X_norm}\n",
    "\n",
    "for i,x in stand.items():\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, train_y, test_size=0.3,random_state=123)\n",
    "    \n",
    "    print('----------------',i,'------------------')\n",
    "\n",
    "    # NAIBE BAYES\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train,y_train.values.ravel())\n",
    "    predicted= model.predict(x_test)\n",
    "    print('Naive Bayes',accuracy_score(y_test, predicted))\n",
    "\n",
    "    #KNN\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    knn_model.fit(x_train,y_train.values.ravel())\n",
    "    predicted= knn_model.predict(x_test)\n",
    "    print('KNN',accuracy_score(y_test, predicted))\n",
    "\n",
    "    #RANDOM FOREST\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "    rfc_model.fit(x_train,y_train.values.ravel())\n",
    "    predicted = rfc_model.predict(x_test)\n",
    "    print('Random Forest',accuracy_score(y_test,predicted))\n",
    "\n",
    "    #LOGISTIC REGRESSION\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    lr_model = LogisticRegression(solver = 'saga')\n",
    "    lr_model.fit(x_train,y_train.values.ravel())\n",
    "    lr_predicted = lr_model.predict(x_test)\n",
    "    print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "\n",
    "    #SVM\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    svc_model = SVC(gamma = 'auto')\n",
    "    svc_model.fit(x_train,y_train.values.ravel())\n",
    "    svc_predicted = svc_model.predict(x_test)\n",
    "    print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "\n",
    "    #DECISON TREE\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    dtree_model = DecisionTreeClassifier()\n",
    "    dtree_model.fit(x_train,y_train.values.ravel())\n",
    "    dtree_predicted = dtree_model.predict(x_test)\n",
    "    print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "\n",
    "    #GradientBOOST\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    # 建立模型\n",
    "    clf = GradientBoostingClassifier()\n",
    "    # 訓練模型\n",
    "    clf.fit(x_train, y_train.values.ravel())\n",
    "    # 預測測試集\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"GradientBoosting\", acc)\n",
    "\n",
    "\n",
    "    #ADABOOST\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    # 建立模型\n",
    "    clf = GradientBoostingClassifier(loss='exponential')\n",
    "    # 訓練模型\n",
    "    clf.fit(x_train, y_train.values.ravel())\n",
    "    # 預測測試集\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"ADABOOST\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- train_x ------------------\n",
      "Naive Bayes 0.8160000000000001\n",
      "KNN 0.906\n",
      "Random Forest 0.8690000000000001\n",
      "Logistic Regression 0.8210000000000001\n",
      "SVM 0.915\n",
      "Decision Tree 0.775\n",
      "GradientBoosting 0.883\n",
      "ADABOOST 0.8700000000000001\n",
      "---------------- X_std ------------------\n",
      "Naive Bayes 0.8160000000000001\n",
      "KNN 0.8149999999999998\n",
      "Random Forest 0.8690000000000001\n",
      "Logistic Regression 0.8219999999999998\n",
      "SVM 0.8720000000000001\n",
      "Decision Tree 0.776\n",
      "GradientBoosting 0.8800000000000001\n",
      "ADABOOST 0.8700000000000001\n",
      "---------------- X_mms ------------------\n",
      "Naive Bayes 0.8160000000000001\n",
      "KNN 0.8230000000000001\n",
      "Random Forest 0.8690000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 0.8210000000000001\n",
      "SVM 0.818\n",
      "Decision Tree 0.781\n",
      "GradientBoosting 0.881\n",
      "ADABOOST 0.8700000000000001\n",
      "---------------- X_norm ------------------\n",
      "Naive Bayes 0.808\n",
      "KNN 0.9019999999999999\n",
      "Random Forest 0.8699999999999999\n",
      "Logistic Regression 0.8220000000000001\n",
      "SVM 0.808\n",
      "Decision Tree 0.796\n",
      "GradientBoosting 0.874\n",
      "ADABOOST 0.8789999999999999\n"
     ]
    }
   ],
   "source": [
    "#用cross_val\n",
    "\n",
    "\n",
    "for i,x in stand.items():\n",
    "    \n",
    "    print('----------------',i,'------------------')\n",
    "\n",
    "    # NAIBE BAYES\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    print('Naive Bayes',cross_val_score(model,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "    #KNN\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    print('KNN',cross_val_score(knn_model,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "    #RANDOM FOREST\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "    print('Random Forest',cross_val_score(rfc_model,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "    #LOGISTIC REGRESSION\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    lr_model = LogisticRegression(solver = 'saga')\n",
    "    print('Logistic Regression',cross_val_score(lr_model,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "    #SVM\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    svc_model = SVC(gamma = 'auto')\n",
    "    print('SVM',cross_val_score(svc_model,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "    #DECISON TREE\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    dtree_model = DecisionTreeClassifier()\n",
    "    print('Decision Tree',cross_val_score(dtree_model,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "    #GradientBOOST\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    # 建立模型\n",
    "    clf = GradientBoostingClassifier()\n",
    "    print(\"GradientBoosting\", cross_val_score(clf,x,train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "\n",
    "    #ADABOOST\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    # 建立模型\n",
    "    clf = GradientBoostingClassifier(loss='exponential')\n",
    "    print(\"ADABOOST\", cross_val_score(clf,x,train_y.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Score 0.874\n",
      "Random Forest Best Parmas {'max_depth': 20, 'n_estimators': 200}\n",
      "Random Forest Accuracy 0.874\n",
      "KNN Best Score 0.911\n",
      "KNN Best Params {'n_neighbors': 3}\n",
      "KNN Accuracy 0.9109999999999999\n",
      "SVM Best Score 0.918\n",
      "SVM Best Params {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "SVM Accuracy 0.9180000000000001\n"
     ]
    }
   ],
   "source": [
    "#最佳化參數   使用未標準化 train_x 原始的\n",
    "\n",
    "#Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=99)\n",
    "\n",
    "#USING GRID SEARCH\n",
    "n_estimators = [10, 50, 100, 200,400]\n",
    "max_depth = [3, 10, 20, 40]\n",
    "param_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\n",
    "\n",
    "grid_search_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 10,scoring='accuracy',n_jobs=-1).fit(train_x, train_y.values.ravel())\n",
    "rfc_best = grid_search_rfc.best_estimator_\n",
    "print('Random Forest Best Score',grid_search_rfc.best_score_)\n",
    "print('Random Forest Best Parmas',grid_search_rfc.best_params_)\n",
    "print('Random Forest Accuracy',cross_val_score(rfc_best,train_x, train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "#KNN \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#USING GRID SEARCH\n",
    "n_neighbors=[3,5,6,7,8,9,10]\n",
    "param_grid = dict(n_neighbors=n_neighbors)\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(train_x,train_y.values.ravel())\n",
    "knn_best = grid_search_knn.best_estimator_\n",
    "print('KNN Best Score', grid_search_knn.best_score_)\n",
    "print('KNN Best Params',grid_search_knn.best_params_)\n",
    "print('KNN Accuracy',cross_val_score(knn_best,train_x, train_y.values.ravel(), cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc = SVC()\n",
    "\n",
    "#USING GRID SEARCH\n",
    "parameters = [{'kernel':['linear'],'C':[1,10,100]},\n",
    "              {'kernel':['rbf'],'C':[1,10,100],'gamma':[0.05,0.0001,0.01,0.001]}]\n",
    "grid_search_svm = GridSearchCV(estimator=svc, param_grid=parameters, cv = 10, n_jobs=-1,scoring='accuracy').fit(train_x, train_y.values.ravel())\n",
    "svm_best = grid_search_svm.best_estimator_\n",
    "print('SVM Best Score',grid_search_svm.best_score_)\n",
    "print('SVM Best Params',grid_search_svm.best_params_)\n",
    "print('SVM Accuracy',cross_val_score(svm_best,train_x, train_y.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Best Score 0.894\n",
      "SVM Best Params {'max_depth': 5, 'n_estimators': 300}\n",
      "SVM Accuracy 0.891\n"
     ]
    }
   ],
   "source": [
    "#GradientBoost\n",
    "\n",
    "# 建立模型\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "n_estimators = [50, 100, 200, 300, 400]\n",
    "max_depth = [1, 2, 3, 4, 5]\n",
    "param_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "grid_search_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(train_x, train_y.values.ravel())\n",
    "clf_best = grid_search_clf.best_estimator_\n",
    "print('SVM Best Score',grid_search_clf.best_score_)\n",
    "print('SVM Best Params',grid_search_clf.best_params_)\n",
    "print('SVM Accuracy',cross_val_score(clf_best,train_x, train_y.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最後選擇SVM\n",
    "\n",
    "##### knn svm gb #####\n",
    "knn_best_pred = pd.DataFrame(knn_best.predict(test_x))\n",
    "svm_best_pred = pd.DataFrame(svm_best.predict(test_x))\n",
    "clf_best_pred = pd.DataFrame(clf_best.predict(test_x))\n",
    "\n",
    "#index從1開始\n",
    "knn_best_pred.index += 1\n",
    "svm_best_pred.index += 1\n",
    "clf_best_pred.index += 1\n",
    "\n",
    "knn_best_pred.columns = ['Solution']\n",
    "svm_best_pred.columns = ['Solution']\n",
    "clf_best_pred.columns = ['Solution']\n",
    "\n",
    "knn_best_pred['Id'] = np.arange(1,knn_best_pred.shape[0]+1)\n",
    "knn_best_pred = knn_best_pred[['Id', 'Solution']]\n",
    "svm_best_pred['Id'] = np.arange(1,svm_best_pred.shape[0]+1)\n",
    "svm_best_pred = svm_best_pred[['Id', 'Solution']]\n",
    "clf_best_pred['Id'] = np.arange(1,clf_best_pred.shape[0]+1)\n",
    "clf_best_pred = clf_best_pred[['Id', 'Solution']]\n",
    "\n",
    "\n",
    "\n",
    "knn_best_pred.to_csv('knn_best_pred.csv', index=False)\n",
    "svm_best_pred.to_csv('svm_best.csv', index=False)\n",
    "clf_best_pred.to_csv('clf_best.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
